{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *`11_Support Vector Machines`*\n",
        "\n",
        "Support vector machine (SVMs) is a supervised machine learning algorithm that classifies data by finding an optimal line or hyperplane that maximizes the distance between each class in an N-dimensional space.\n",
        "\n",
        "The core idea is to classify the points as widely as possible. Maximize the distance between points.\n",
        "\n",
        "<image>\n",
        "\n",
        "We need to select a margin maximizing hyperplane.\n",
        "1. We randomly select a $\\pi$ hyperplane.\n",
        "2. We then draw hyperplanes parallel to $\\pi$ till we touch the first point in positive direction.\n",
        "3. We then draw hyperplane parallel to $\\pi$ till we touch the first point in negative direction.\n",
        "\n",
        "  $margin_{distance} = \\text  {distance between }  \\pi^+ \\text { and  } \\pi^- $.\n",
        "\n",
        "4. We repeat the same process by drawing hyperplanes (all possible combinations) and calculating the margin. You then select that hyperplane whose margin is maximum.\n",
        "\n",
        "$$ W^T X + b$$\n",
        "\n",
        "5. We are actually trying to find out $W$ and $b$ whose $margin_{distance}$ will be maximum.\n",
        "\n",
        "### Support Vectors\n",
        "\n",
        "The first point which crosses the parallel hyperplanes in both negative and positive direction.\n",
        "\n",
        "<image>\n",
        "\n",
        "1. Support Vectors are robust to outliers.\n",
        "2. It works well for non linear data as well.\n",
        "3. SVM are useful for regression as well as classification.\n",
        "\n",
        "#### Mathematics of SVM\n",
        "\n",
        "<image>\n",
        "\n",
        "We are going to find the decision rule for new data points to classify. Draw a vector $\\vec w$ perpendicular to hyperplane and $\\vec u$ is the data point we want to find.\n",
        "\n",
        "$\\vec w . \\vec u \\geq c\n",
        "\\\\\n",
        "\\vec w . \\vec u - c \\geq 0\n",
        "\\\\\n",
        "\\vec w . \\vec u + b \\geq 0 \\text {   .... decision rule }$\n",
        "\n",
        "\n",
        "$2x + 3y + 3 = 0$ --> for lines as well we follow the same principle, if $(x, y)$ point when put into line equation is greater than $0$ then it is positive region else negative region.\n",
        "\n",
        "<image>\n",
        "\n",
        "\n",
        "We have certain assumptions for thw $\\pi ^ + \\text {and } \\pi ^ -$ hyperplanes.\n",
        "$$\\pi = w^T x + b$$\n",
        "\n",
        "Assumptions:\n",
        "* $\\pi^+ = w^T x + b = 1$\n",
        "* $\\pi^- = w^T x + b = -1$\n",
        "\n",
        "<image>\n",
        "\n",
        "> **Why the equations should be equal to 1 and -1?**\\\n",
        "The answer to this is we want to find $d$ where, $d_1$ should be exactly equal to $d_2$. Therefore, $1$ and $-1$.\n",
        "\n",
        "<image>\n",
        "\n",
        "**We need to calculate $d$**\n",
        "\n",
        "We have a constraint with our existing points that they should never cross the $\\pi ^ + \\text {and } \\pi ^ -$ hyperplane i.e:\n",
        " * Red points should not go above $\\pi ^ -$.\n",
        " * Green points should not go below $\\pi ^ +$.\n",
        "\n",
        "Contraint in mathematical form:\n",
        " 1. $w^T x + b \\geq 1$  -- Green points\n",
        " 2. $w^T x + b \\leq -1$  -- Red points\n",
        "\n",
        "For convenience, we will make an equation from above 2 equations.\n",
        "\n",
        "$-$ We will assume, Green points label = $+1$\n",
        "$-$ We will assume, Red points label = $-1$\n",
        "\n",
        "So,\n",
        "for $+1$:\n",
        "* $y_i (\\vec w . \\vec x_i + b) \\geq 1$\n",
        "\n",
        "for $-1$:\n",
        "* $y_i (\\vec w . \\vec x_i + b) \\geq 1$\n",
        "\n",
        "\\\n",
        "But since $y_i(-1)$ then sign will reverse so, we get 1 equation for both cases:\n",
        "$y_i (\\vec w . \\vec x_i + b) \\geq 1$\n",
        "\n",
        "\\\n",
        "*For support vectors* the equation will be:\n",
        "$y_i (\\vec w . \\vec x_i + b) = 0$\n",
        "\n",
        "\n",
        "The distance $d$ will be calculated until the equations we formed and the condition that follows them is true.\n",
        "\n",
        "<image>\n",
        "\n",
        "\n",
        "We had initialized one vector $\\vec w$ perpendicular to the hyperplane. Then, if we project $x_1 \\text {and } x_2$ on it we get $d$. And $\\vec w$ should be a unit vector so, $\\frac {\\vec w}{||W||}$\n",
        "\n",
        "$d = (x_2 - x_1). \\frac {\\vec w}{||W||} = \\frac {\\vec x_2 \\vec w - \\vec x_1 \\vec w}{||W||}$\n",
        "\n",
        "$y_i (\\vec w . \\vec x_i + b) = 1 $\n",
        "\n",
        "* $ 1 (\\vec w . \\vec x_2 + b) = 1\n",
        "\\\\\n",
        "=> \\vec w \\vec x_2 = 1 - b \\text { ... for } x_2$\n",
        "\n",
        "\\\n",
        "* $-1 (\\vec w . \\vec x_i + b) = 1\n",
        "\\\\\n",
        "=> 1 (\\vec w . \\vec x_1 + b) = 1\n",
        "\\\\\n",
        "=> -(\\vec w . \\vec x_1 - b) = 1\n",
        "=> \\vec w . \\vec x_1 = - b - 1 \\text { ... for } x_1$\n",
        "\n",
        "Putting both values in equation 1:\n",
        "  \n",
        "$$\n",
        "margin_{distance} = d = \\frac {1 - b - (-b - 1)}{||W||}\n",
        "\\\\\n",
        "= \\frac {1 -(- 1))}{||W||} = \\frac {2}{||W||}$$\n",
        "\n",
        "\n",
        "\n",
        "*This is the optimization formula:*\n",
        "\n",
        "$$[d = argmin_{w^*, b^*} \\frac {2}{||W||}]_{\\text {, such that: } y_i.(\\vec w . \\vec x_i + b) \\geq 1}$$\n",
        "\n",
        "\\\n",
        "> **Hard Margin Vs. Soft Margin SVM**\n",
        "\n",
        "**Hard Margin SVM**\n",
        "\n",
        "The hard margin SVM works for purely linearly separates data. And so, if any impurity is present in data it will not work.\n",
        "\n",
        "\\\n",
        "**Soft Margin SVM**\n",
        "\n",
        "In real world we won't have purely linearly separable data. We are bound to have some impurity in the data. In that case, Hard Margin SVM would not work. That's when Soft Margin SVM comes into play.\n",
        "\n",
        "Soft Margin SVM will allow outliers in the data. If we are trying to maximize $f(x)$ it is equivalent to minimizing $\\frac {1}{f(x)}$.\n",
        "\n",
        "$max (f(x)) = min \\frac {1}{f(x)}$\n",
        "\n",
        "therefore, we can write:\n",
        "\n",
        "$argmin_{w^*, b^*} \\frac {||W||}{2}$ ... Reciprocal of Hard margin SVM\n",
        "\n",
        "\n",
        "$$SVM_{softmargin} = argmin_{w^*, b^*} \\frac {||W||}{2} + \\sum_{i=1}^{n} \\zeta_i$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> **SVM Error**\n",
        "\n",
        "Let's consider the following scenario. When we are training SVM, we are always talking about the points in terms of $\\pi^+ \\text {and } \\pi^-$.\n",
        "\n",
        "We only refer to $\\pi$ when we are trying to predict the new data point.\n",
        "\n",
        "``\n",
        "The incorrect classified data points/outliers/error points will have Î¶ value\n",
        "``\n",
        "\n",
        "* All -ve points below, $\\pi^-, \\zeta_i = 0$.\n",
        "* All +ve points above, $\\pi^+, \\zeta_i = 0$.\n",
        "\n",
        "1. The circled points are incorrectly classified so, they will have $\\zeta$ value.\n",
        "\n",
        "<images>\n",
        "\n",
        "2. These distances of the incorrect points to positive or negative hyperplane are $\\zeta_i$ and we want to minimize these distances.\n",
        "  \n",
        "  The error in SVM is calculated as:\n",
        "\n",
        "$$SVM_{error} = Margin_{error} + classification_{error}$$\n",
        "  \n",
        "  Where,\n",
        "\n",
        "* Margin error is given as:\n",
        "\n",
        "  $$margin_{error} = argmin_{w^*, b^*} \\frac {||W||}{2}$$\n",
        "\n",
        "* Classification error is  given by:\n",
        "\n",
        "  $$classification_{error} = \\zeta_i * C$$\n",
        "  \n",
        "  $\\zeta_i$ --> will be calculated for each data point which is ***incorrectly classified***. It will be $0$ for correctly classified points.\n",
        "\n",
        "$-$ The more the margin the lower will be the margin error.\n",
        "\n",
        "$-$ The lower the margin the greater will be the margin error.\n",
        "\n",
        "<image>\n",
        "\n",
        "1. Here, the $Model_1$ is having a wider margin which desirable. but it is making some errors which is not desirable.\n",
        "2. On the other hand, $Model_2$ is having a thinner margin but it is not making any classification error.\n",
        "\n",
        "Which model is desirable is hard to tell, but if we use below formula, we can justify it:\n",
        "\n",
        "$margin_{error} = argmin_{w^*, b^*} \\frac {||W||}{2} + \\sum_{i=1}^{n} \\zeta_i * C$\n",
        "\n",
        "where,  $C$ is a hyperparameter.\n",
        "\n",
        "3. Setting $C$ value high will mean that, the focus is on minimizing the classification error while margin can be maximized.\n",
        "\n",
        "  * $\\frac {||W||}{2}$ : maximizing margin\n",
        "  * $\\zeta_i : classification error$\n",
        "\n",
        "4. Setting $C$ value low will mean to focus is on minimizing the margin error. We can compare SVM loss with logistic error:\n",
        "\n",
        "  $Logistic Loss + \\lambda ||W||$\n",
        "  \n",
        "  where, $||W||$ is regularization term.\n",
        "\n",
        "  Similarly,\n",
        "  $$SVM_{loss} = argmin_{w^*, b^*} \\frac {||W||}{2} + \\sum_{i=1}^{n} \\zeta_i * C$$\n",
        "\n",
        "  Where,\n",
        "  \n",
        "  * $argmin_{w^*, b^* } \\frac {||W||}{2}$ is regularization term.\n",
        "\n",
        "  * $C * \\sum_{i=1}^{n} \\zeta_i$ is Hinge loss.\n",
        "\n",
        "  * $C \\propto \\frac {1}{\\lambda}$\n",
        "\n",
        "* Since, we are allowing some errors to take place. Therefore, it is called  **Soft Margin SVM.**\n",
        "\n",
        "\\\n",
        "> **Kernel Trick in SVM**\n",
        "\n",
        "<image>\n",
        "\n",
        "In kernel trick, we convert $1D$ data into $2D$ data. We find a function which can transform $1D$ data into $2D$ data.\n",
        "\n",
        "$$k( \\vec x, \\vec l{^i}) = e^{-\\frac{||\\vec x - \\vec {l^i}||^2}{2\\sigma^2}}$$\n",
        "\n",
        "The values inside the kernel will be assigned values greater than $0$. The values outside the kernel will be assigned to $0$.\n",
        "\n",
        "\n",
        "> **Kernel Transformations**\n",
        "\n",
        "Below are the Kernel transformations that we can apply on our data.\n",
        "\n",
        " * RBF : Radial Basis Function/Gaussian : $k( \\vec x, \\vec l{^i}) = e^{-\\frac{||\\vec x - \\vec {l^i}||^2}{2\\sigma^2}}$\n",
        "\n",
        " * Polynomial kernel: $k(x, y) = tan (\\gamma. x^Ty + r)$\n",
        "\n",
        " * Sigmoid kernel: $k(x, y) = tan (\\gamma. x^Ty + r)^d , \\gamma \\gt 0$\n",
        "\n",
        "\\\n",
        "RBF function applied to data :\n",
        "\n",
        "<image>\n",
        "\n",
        "It lifts the data points at the centre.\n"
      ],
      "metadata": {
        "id": "4Rs1trD4ADTi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NeLHi5nkaM0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}